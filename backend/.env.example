# API Server configuration
API_HOST=0.0.0.0
API_PORT=8000

# CORS configuration - comma-separated list of allowed origins
CORS_ORIGINS=http://localhost:3000,https://yourapp.com

# OpenAI configuration
OPENAI_API_KEY=your_openai_api_key

# =============================================================================
# MODEL CONFIGURATION
# Configure different models for different purposes to optimize cost vs capability
# =============================================================================

# Main conversation model - used for chat responses, integration, and response rendering
# Recommendation: Use a capable model (gpt-4o, gpt-4-turbo) for best user experience
# Cost impact: High (most frequent usage)
DEFAULT_MODEL=gpt-4o-mini

# Router model - lightweight model for routing decisions (chat vs search vs analyzer)
# Recommendation: Use a fast, cheap model (gpt-4o-mini, gpt-3.5-turbo) since routing is simple classification
# Cost impact: Low (quick decisions only)
ROUTER_MODEL=gpt-4o-mini

# Research model - used for autonomous background research workflow
# Recommendation: Use a capable analytical model (gpt-4o, gpt-4-turbo) for thorough research
# Cost impact: Medium (background tasks, less frequent than chat)
RESEARCH_MODEL=gpt-4o-mini

# Topic extraction model - extracts research-worthy topics from conversations
# Recommendation: Use a structured output capable model (gpt-4o-mini works well)
# Cost impact: Low (structured output tasks)
TOPIC_EXTRACTION_MODEL=gpt-4o-mini

# =============================================================================
# EXTERNAL API CONFIGURATION
# =============================================================================

# Perplexity configuration for web search
PERPLEXITY_API_KEY=your_perplexity_api_key
# Perplexity model for web search - use 'sonar' for their default search model
PERPLEXITY_MODEL=sonar

# PubMed E-utilities configuration (no API key required, but email recommended)
# Email helps NCBI contact you if there are issues with your requests
PUBMED_EMAIL=your_email@example.com

# Zep Memory Configuration (optional)
ZEP_API_KEY=your_zep_api_key_here
ZEP_ENABLED=false

# Langchain configs
LANGCHAIN_TRACING_V2=True
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=your_langchain_api_key
LANGCHAIN_PROJECT=researcher-prototype

# =============================================================================
# SYSTEM CONFIGURATION
# =============================================================================

# Logging configuration
# Available levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Message management configuration
MAX_MESSAGES_IN_STATE=4

# Topic extraction configuration
TOPIC_MIN_CONFIDENCE=0.8
TOPIC_MAX_SUGGESTIONS=3
TOPIC_EXTRACTION_TEMPERATURE=0.3
TOPIC_EXTRACTION_MAX_TOKENS=800

# Autonomous Research Engine configuration
RESEARCH_ENGINE_ENABLED=true
RESEARCH_QUALITY_THRESHOLD=0.6
RESEARCH_MAX_TOPICS_PER_USER=3
RESEARCH_FINDINGS_RETENTION_DAYS=30

# Motivation system configuration
MOTIVATION_CHECK_INTERVAL=60
MOTIVATION_THRESHOLD=5.0
MOTIVATION_BOREDOM_RATE=0.0002
MOTIVATION_CURIOSITY_DECAY=0.0001
MOTIVATION_TIREDNESS_DECAY=0.0001
MOTIVATION_SATISFACTION_DECAY=0.0001

# Per-topic motivation configuration
TOPIC_MOTIVATION_THRESHOLD=0.5
TOPIC_ENGAGEMENT_WEIGHT=0.3
TOPIC_QUALITY_WEIGHT=0.2
TOPIC_STALENESS_SCALE=0.0001

# =============================================================================
# TOPIC EXPANSION CONFIGURATION
# Configure the LLM-powered topic expansion system
# =============================================================================

# Topic expansion LLM confidence threshold (0.0-1.0)
# Higher values = more selective, lower values = more topics generated
EXPANSION_LLM_CONFIDENCE_THRESHOLD=0.6

# =============================================================================
# ENGAGEMENT SCORING CONFIGURATION  
# Configure how user engagement with research is calculated
# =============================================================================

# Weights for different engagement signals
ENGAGEMENT_RESEARCH_WEIGHT=2.0      # Heavy weight for research findings interaction
ENGAGEMENT_ANALYTICS_WEIGHT=0.5     # Lower weight for general analytics

# Bonus rates and maximums for engagement scoring
ENGAGEMENT_RECENT_BONUS_RATE=0.2    # Bonus per recent read
ENGAGEMENT_RECENT_BONUS_MAX=0.5     # Maximum recent engagement bonus
ENGAGEMENT_VOLUME_BONUS_RATE=0.1    # Bonus per total finding
ENGAGEMENT_VOLUME_BONUS_MAX=0.3     # Maximum volume bonus  
ENGAGEMENT_BOOKMARK_BONUS_RATE=0.15 # Bonus per bookmark
ENGAGEMENT_BOOKMARK_BONUS_MAX=0.45  # Maximum bookmark bonus
ENGAGEMENT_INTEGRATION_BONUS_RATE=0.2 # Bonus per integration
ENGAGEMENT_INTEGRATION_BONUS_MAX=0.6  # Maximum integration bonus
ENGAGEMENT_SCORE_MAX=2.0            # Cap total engagement score

# =============================================================================
# RESEARCH TIMING CONFIGURATION
# Configure delays and timing for autonomous research engine
# =============================================================================

RESEARCH_CYCLE_SLEEP_INTERVAL=300   # Sleep between research cycles (seconds)
RESEARCH_TOPIC_DELAY=1.0           # Delay between topics (seconds)
RESEARCH_MANUAL_DELAY=0.5          # Delay in manual research (seconds)
RESEARCH_MAX_TOKENS=2000           # Max tokens for research LLM calls

# =============================================================================
# SYSTEM PERFORMANCE CONFIGURATION
# =============================================================================

# Status update throttling
STATUS_MIN_INTERVAL=0.3            # Minimum seconds between status updates

# Admin Interface Configuration (CHANGE THESE IN PRODUCTION!)
ADMIN_PASSWORD=admin123
ADMIN_JWT_SECRET=your-secret-key-change-in-production
ADMIN_JWT_EXPIRE_MINUTES=480
